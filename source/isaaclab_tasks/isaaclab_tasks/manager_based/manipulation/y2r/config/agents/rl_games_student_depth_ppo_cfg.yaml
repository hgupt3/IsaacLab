# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# Copyright (c) 2024, Harsh Gupta
# All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause

# =============================================================================
# Student Policy Configuration for DaGGer/BC Distillation and PPO Fine-tuning
# =============================================================================
# Network architecture config for student policy distillation and RL fine-tuning.
# 
# Inputs:
#   - Wrist camera depth (64x64) → SmallResNet → 128-dim features
#   - Visible point clouds (student_current_pc, student_future_pc)
#   - Proprioceptive observations (joint pos, vel, tips)
#
# Teacher uses: perception, targets (privileged full point clouds)
# Student uses: student_current_pc, student_future_pc (visible point clouds only)
#
# Depth is packed into obs (appended at end) so standard train.py works.
# The model slices depth from obs tail, applies optional DepthAug, and encodes.
# =============================================================================

params:
  seed: 42

  # Environment wrapper clipping
  env:
    clip_observations: 100.0
    clip_actions: 100.0
    
    # Observation groups for student policy
    obs_groups:
      obs: ["policy", "student_proprio", "student_current_poses", "student_future_poses", "student_current_pc", "student_future_pc", "student_camera"]
      states: ["policy", "proprio", "current_poses", "future_poses", "current_pc", "future_pc"]  
    concate_obs_groups: True

  algo:
    name: a2c_continuous

  model:
    name: continuous_a2c_logstd

  # ==========================================================================
  # NETWORK ARCHITECTURE
  # ==========================================================================
  network:
    name: depth_resnet_student
    separate: False

    # IMPORTANT:
    # We pack depth into obs, so RL-Games' global RunningMeanStd (config.normalize_input)
    # would incorrectly normalize depth. We therefore disable RL-Games normalize_input
    # and do base_obs-only normalization inside the model instead.
    normalize_base_obs: True
    
    # Depth encoder: SmallResNet for 64x64 depth images
    depth_encoder:
      resolution: 64
      channels: [32, 128, 256]
    
    # Depth augmentation (GPU-accelerated via NVIDIA Warp)
    # Applied INSIDE the model during training only (self.training=True)
    depth_augmentation: True
    depth_aug_config:
      correlated_noise:
        sigma_s: 0.3              # Spatial noise
        sigma_d: 0.1              # Depth noise
      pixel_dropout_and_randu:
        p_dropout: 0.003          # Dropout probability
        p_randu: 0.003            # Random uniform probability
      sticks:
        p_stick: 0.00025          # Stick artifact probability
        max_stick_len: 12.0       # Max stick length (pixels)
        max_stick_width: 2.0      # Max stick width (pixels)
    
    space:
      continuous:
        mu_activation: None
        sigma_activation: None
        mu_init:
          name: default
        sigma_init:
          name: const_initializer
          val: 0
        fixed_sigma: True
    
    mlp:
      units: [512, 256, 128]
      activation: elu
      d2rl: False
      initializer:
        name: default
      regularizer:
        name: None

  load_checkpoint: False
  load_path: ''

  # ==========================================================================
  # RL-GAMES CONFIG (matches teacher config structure)
  # ==========================================================================
  config:
    name: student_depth_distillation
    env_name: rlgpu
    device: 'cuda:0'
    device_name: 'cuda:0'
    multi_gpu: False
    ppo: True
    mixed_precision: True
    normalize_input: False
    normalize_value: True
    value_bootstrap: False
    num_actors: -1
    reward_shaper:
      scale_value: 0.01
    normalize_advantage: True
    gamma: 0.99
    tau: 0.95
    learning_rate: 1e-3
    lr_schedule: adaptive
    schedule_type: legacy
    kl_threshold: 0.01
    score_to_win: 100000000
    max_epochs: 750000
    save_best_after: 1000
    save_frequency: 500
    print_stats: True
    grad_norm: 1.0
    entropy_coef: 0.002
    truncate_grads: True
    e_clip: 0.2
    horizon_length: 36
    minibatch_size: 4608
    mini_epochs: 5
    critic_coef: 4
    clip_value: True
    clip_actions: False
    seq_len: 4
    bounds_loss_coef: 0.0001

  # ==========================================================================
  # DISTILLATION SETTINGS (used by DistillAgent)
  # Note: seq_length from config: section is used for RNN truncated BPTT
  # Note: Depth augmentation is handled INSIDE the model (see network.depth_augmentation)
  # ==========================================================================
  distillation:
    # Training
    learning_rate: 1e-4           # Adam lr for distillation
    grad_norm: 1.0                # Gradient clipping
    max_iterations: 100000000     # Total training iterations
    save_frequency: 5000          # Checkpoint save interval (iterations, not frames)
    save_best_after: 10000        # Don't save "best" until this many epochs/iters (avoid early noise)
    
    # Value distillation (train critic from teacher value)
    value_distillation: True      # Enable value distillation (MSE loss on critic)
    
    # DaGGer
    beta: 0.5                     # Fraction of envs using teacher (0.5 = 50%)
    
    # Infrastructure
    mixed_precision: True         # Use AMP for faster training
    normalize_input: True         # Normalize observations with RunningMeanStd
    multi_gpu: False              # DDP (overridden by --distributed flag)

    # Debug: save depth video for env 0
    # Output path: logs/.../nn/depth_video.mp4
    debug_depth_video: False
    debug_depth_video_fps: 30
    debug_depth_video_num_frames: 100  # Record this many frames then save
