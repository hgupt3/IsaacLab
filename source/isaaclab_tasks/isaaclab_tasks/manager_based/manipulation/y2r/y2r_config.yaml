# ==============================================================================
# Y2R Trajectory Task Configuration
# ==============================================================================
# All configuration for the Y2R trajectory following task.
# Edit this file to change task behavior. Set reward weights to 0 to disable.
# Format: [min, max] for ranges, [initial, final] for curriculum interpolation.
# ==============================================================================

# ==============================================================================
# SIMULATION
# ==============================================================================
simulation:
  physics_dt: 0.01              # 1/100 seconds (100 Hz physics)
  decimation: 2                 # Control at 60 Hz (physics_dt * decimation)
  num_envs: 16384
  env_spacing: 3.0
  replicate_physics: false

# ==============================================================================
# MODE SELECTION
# ==============================================================================
mode:
  use_point_cloud: false        # true = point cloud tracking, false = pose tracking
  use_eigen_grasp: true         # true = 28D eigen grasp, false = 23D full joints
  use_student_mode: false       # true = adds student_perception and student_targets obs groups (point cloud only)

# ==============================================================================
# OBSERVATIONS
# ==============================================================================
observations:
  num_points: 64                # Points sampled per object surface
  clip_range: [-2.0, 2.0]       # Clip observation values
  
  # History lengths for observation groups
  history:
    policy: 5                   # Action history
    proprio: 5                  # Joint pos/vel, hand tips, contact
    perception: 5               # Object point cloud and pose history
    targets: 1                  # No history (targets already encode future)

# ==============================================================================
# TRAJECTORY TIMING
# ==============================================================================
trajectory:
  duration: 9.0                # Total episode length (seconds)
  target_hz: 6.667              # Target update rate (20/3 Hz = 150ms between targets)
  window_size: 5                # Number of lookahead targets in observation
  
  # Phase durations (seconds) - must sum to duration
  phases:
    pickup: 0.5                 # Stationary at start for grasping
    manipulate: 8.0             # Main movement phase with easing
    release: 0.5                # Stationary at goal for release
  
  easing_power: 1.0             # Quadratic ease curves (higher = sharper transitions)

# ==============================================================================
# WORKSPACE BOUNDS
# ==============================================================================
# Relative to robot base frame (meters)
workspace:
  x: [-0.75, -0.35]
  y: [-0.25, 0.25]
  z: [0.28, 0.65]               # [min, max] - above table surface
  table_surface_z: 0.255        # Height of table surface

# ==============================================================================
# WAYPOINTS
# ==============================================================================
# Intermediate points between start and goal (0-2 randomly sampled)
waypoints:
  count: [0, 2]                 # [min, max] waypoints per trajectory
  
  # Position sampling (null = use workspace bounds, else offset from start)
  position_range:
    x: null                     # null = sample from workspace.x
    y: null                     # null = sample from workspace.y
    z: null                     # null = sample from workspace.z
  
  # Orientation randomization
  vary_orientation: true
  max_rotation: 1.4            # Max rotation in radians (~114 degrees)

# ==============================================================================
# GOAL SAMPLING
# ==============================================================================
# Goal is always on table surface with stable orientation
goal:
  x_range: null                 # null = use workspace.x
  y_range: null                 # null = use workspace.y
  table_margin: 0.0             # Keep goal this far from table edge

# ==============================================================================
# REWARDS
# ==============================================================================
# Set weight to 0 to disable any reward
# Thresholds use [initial, final] format - curriculum interpolates between
rewards:
  action_l2:
    weight: -0.005
    finger_scale: 0.1         # 10x less penalty on hand actions
    
  action_rate_l2:
    weight: -0.005
    finger_scale: 0.1         # 10x less penalty on hand actions
    
  fingers_to_object:
    weight: 2.0
    std: 0.4
    # When tracking error is high, reduce this reward so the policy doesn't
    # prefer "press + squeeze" instead of recovering trajectory tracking.
    # Set to null to disable gating.
    error_gate_pos_threshold: 0.10   # meters (pose mode) / mean error (pc mode)
    error_gate_pos_slope: 0.02       # meters; smaller = sharper gate
    error_gate_rot_threshold: 1.0    # radians (pose mode only); set null to ignore rotation
    error_gate_rot_slope: 0.5        # radians
    
  lookahead_tracking:
    weight: 5.0
    std: 0.03                   # Position/point cloud error std for positive reward
    rot_std: 0.3                # Rotation error std (pose mode only) 
    decay: 0.3                  # Exponential decay for lookahead weights
    contact_threshold: 1.0      # Currently unused
    # Soft contact gating: scales tracking reward but never hard-zeros it.
    contact_ramp: 1.0           # Newtons; contact_factor ramps from 0→1 over this range
    contact_min_factor: 0.05    # lower bound so reward doesn't fully collapse
    # Negative penalty zones (activates when error > threshold):
    neg_scale: 0.2              # Max penalty magnitude (0.5 = can go to -0.5)
    neg_threshold: 0.04         # Position error > 8cm triggers penalty
    neg_std: 0.1                # How fast position penalty grows
    rot_neg_threshold: 0.4      # Rotation error > 0.6 rad (~34°) triggers penalty
    rot_neg_std: 1.0            # How fast rotation penalty grows
    
  trajectory_success:
    weight: 10.0
    # Success thresholds [initial (lenient), final (strict)]
    point_cloud_threshold: [0.05, 0.05]     # meters
    position_threshold: [0.05, 0.05]        # meters (pose mode)
    rotation_threshold: [0.5, 0.5]          # radians (pose mode) ~34° → ~11°
    
  early_termination:
    weight: -4.0
    term_keys: ["abnormal_robot", "trajectory_deviation"]
    
  arm_table_penalty:
    weight: -0.5
    table_z: 0.255
    threshold_mid: 0.07         # Safe height above table for links 3,4,5
    threshold_distal: 0.04      # Safe height above table for links 6,7,palm
    
  # Robot-specific (Kuka Allegro)
  good_finger_contact:
    weight: 0.5
    threshold: 1.0              # Contact force threshold (Newtons)
    
  finger_manipulation:
    weight: 4.0                 # Shaping reward - encourages in-hand adjustment
    pos_std: 0.02               # Position change threshold (meters)
    rot_std: 0.2                # Rotation change threshold (radians)
    
  distal_joint3_penalty:
    weight: -0.3                # Weak regularizer to discourage over-curling distal joints
    std: 1.6                    # (rad) larger = more lenient
    joint_name_regex: ".*_joint_3"
    only_when_contact: false
    contact_threshold: 1.0
    only_in_manipulation: false
    
  palm_velocity_penalty:
    weight: -0.3                # Negative = penalty during manipulation phase
    angular_std: 0.7            # Angular velocity threshold (rad/s) - stricter
    linear_std: 1.0             # Linear velocity threshold (m/s)
    linear_scale: 0.1           # 5x more lenient on linear than angular
    
  palm_orientation_penalty:
    weight: -0.5                # Weak penalty to avoid awkward wrist configs
    std: 1.0                    # Angular deviation std (rad) - lenient

  # Penalize being too close to joint soft limits (encourages "comfort zone" postures)
  joint_limits_margin:
    weight: -0.05
    threshold: 0.95             # in (0,1): 0.95 means penalize last 5% near either limit
    power: 2.0                  # >=1: higher = sharper penalty near limits

  # Rewards *reducing* current target error (helps recovery if policy freezes).
  tracking_progress:
    weight: 2.0
    pos_weight: 1.0
    rot_weight: 1.0
    positive_only: true
    clip: 1.0

# ==============================================================================
# TERMINATIONS
# ==============================================================================
# Thresholds use [initial, final] format - curriculum interpolates between
terminations:
  trajectory_deviation:
    # Max allowed deviation before episode terminates
    point_cloud_threshold: [0.18, 0.06]     # meters
    position_threshold: [0.4, 0.1] #[0.25, 0.05]        # meters (pose mode)
    rotation_threshold: [2.5, 1.0] #[2.5, 0.5]          # radians (pose mode) ~143° → ~29°

# ==============================================================================
# CURRICULUM (Adaptive Domain Randomization)
# ==============================================================================
curriculum:
  difficulty:
    initial: 0                  # Starting difficulty
    min: 0
    max: 10
    
  # Tolerances to advance difficulty level
  # null = use current success threshold (adaptive with curriculum)
  advancement_tolerances:
    position: null              # null = use success threshold (meters)
    rotation: null              # null = use success threshold (radians)
    point_cloud: null           # null = use success threshold (meters)
    
  # Observation noise curriculum: [initial, final]
  noise:
    joint_pos: [0.0, 0.1]
    joint_vel: [0.0, 0.2]
    hand_tips: [0.0, 0.01]
    object_point_cloud: [0.0, 0.01]
    object_pose: [0.0, 0.01]
    target_point_clouds: [0.0, 0.01]
    target_poses: [0.0, 0.01]
    
  # Gravity scheduling (m/s²)
  gravity:
    initial: [0.0, 0.0, -0.0]
    final: [0.0, 0.0, -9.81]

# ==============================================================================
# DOMAIN RANDOMIZATION
# ==============================================================================
# All ranges are [min, max]
randomization:
  object:
    scale: [0.75, 1.25]
    mass_scale: [0.2, 2.0]
    static_friction: [0.5, 1.0]
    dynamic_friction: [0.5, 1.0]
    restitution: [0.0, 0.0]
    
  robot:
    static_friction: [0.5, 1.0]
    dynamic_friction: [0.5, 1.0]
    restitution: [0.0, 0.0]
    stiffness_scale: [0.5, 2.0]
    damping_scale: [0.5, 2.0]
    joint_friction_scale: [0.0, 5.0]
    
  reset:
    table_xy: [-0.05, 0.05]     # Table position randomization
    object_x: [-0.25, 0.25]     # Object X relative to init
    object_y: [-0.3, 0.3]       # Object Y relative to init
    object_yaw: [-3.14, 3.14]   # Object yaw rotation
    robot_joints: [-0.5, 0.5]   # Joint position offset
    robot_wrist: [-3.0, 3.0]    # Wrist joint (larger range)
    z_offset: 0.0005            # Safety margin above table

# ==============================================================================
# VISUALIZATION
# ==============================================================================
# Set all to false for maximum performance during training
visualization:
  targets: false                 # Show target point clouds
  current_object: false          # Show current object point cloud
  waypoint_region: false         # Show waypoint sampling box
  goal_region: false             # Show goal sampling region on table
  pose_axes: false               # Show axis arrows for poses
  env_ids: []                   # Which envs to visualize ([] = none, null = all)
  debug_print_rewards: false    # Print reward breakdown every 10 steps

# ==============================================================================
# ROBOT-SPECIFIC SETTINGS (Kuka Allegro)
# ==============================================================================
robot:
  action_scale: 0.1
  arm_joint_count: 7
  hand_joint_count: 16
  eigen_dim: 1                  # PCA dimensions for eigen grasp (1..5 supported)

# ==============================================================================
# PUSH-T MODE (disabled by default)
# ==============================================================================
# When enabled, uses T-shaped object with direct-to-goal trajectory and replanning
push_t:
  enabled: false
  object_usd: "assets/t-shape/t-shape.usd"  # Path to T-shape USD (relative to y2r folder)
  outline_usd: "assets/t-shape_outline/t-shape_outline.usd"  # Path to outline USD for goal visualization
  object_scale: 1.0             # Scale for both object and outline
  outline_position: [-0.55, 0.0]  # Fixed goal position on table
  rolling_window: 3.0           # Trajectory planning horizon (seconds)
  min_speed: 0.02               # Minimum speed (m/s) to prevent infinite slowdown
  include_in_primitives: true   # Include T-shape in random object selection pool

# ==============================================================================
# PROCEDURAL OBJECTS
# ==============================================================================
# Generate random "grown" objects by hierarchically attaching primitives
# Shapes are cached in asset_dir; regenerate only if regenerate=true or count changed
procedural_objects:
  enabled: true
  percentage: 0.4                 # 40% procedural, 60% built-in primitives
  asset_dir: "assets/procedural"  # Relative to y2r folder, auto-created
  regenerate: false               # Force regenerate even if shapes exist
  
  # Generation parameters
  generation:
    num_shapes: 100               # How many unique shapes to generate
    primitives_per_shape: [2, 5]  # [min, max] primitives per object
    base_size: [0.035, 0.075]       # [min, max] base primitive size (meters)
    size_decay: 0.7               # Each child primitive is this fraction of parent
    primitive_types:              # Weighted selection of primitive types
      ellipsoid: 0.1
      box: 0.5
      capsule: 0.1
      cylinder: 0.3
    seed: null                    # Random seed (null = different each generation)
